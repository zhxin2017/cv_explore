{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c8b6719-2d1c-40fb-aee5-ce7210e32102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from od import detr_dataset as ds\n",
    "from common.config import val_annotation_file, val_img_od_dict_file, val_img_dir, train_annotation_file, train_img_od_dict_file, train_img_dir, img_size\n",
    "from od.config import cid_to_name\n",
    "from torchvision import transforms\n",
    "from od.box import cxcywh_to_xyxy\n",
    "from od import detr_model, examine, anno, anchor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1966613d-4ae7-4ac3-9a2e-710310622675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_val = anno.build_img_dict(val_annotation_file, val_img_od_dict_file, task='od')\n",
    "dicts_train = anno.build_img_dict(train_annotation_file, train_img_od_dict_file, task='od')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "180f41c3-6d91-458b-84d2-86f7761f2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "anchors = torch.tensor(anchor.generate_anchors(), device=device)\n",
    "model = detr_model.DETR(d_enc=384, d_coord_emb=32, n_enc_head=6, n_dec_head=6, n_enc_layer=24, n_dec_layer=8,\n",
    "                        anchors=anchors, exam_diff=True)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e69064-c9d7-4b4f-8db0-3bb4448c7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'/Users/zx/Documents/ml/restart/resources/od_detr_12.pt'))\n",
    "extractor = examine.FeatureExtractor(model, layers=[m[0] for m in model.named_modules()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d18cf7-ab68-44e2-ba0a-42d7ddd40879",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zx/miniforge3/envs/pytorch/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/zx/Documents/ml/restart/notebook/../od/box.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  bboxes_padded[:n_bboxes] = torch.tensor(bboxes)\n"
     ]
    }
   ],
   "source": [
    "n_head = 6\n",
    "\n",
    "data = ds.OdDataset(dicts_val, n_query=model.decoder.n_anchor, train=False, sample_num=10, random_shift=False)\n",
    "dl = DataLoader(data, batch_size=1, shuffle=True)\n",
    "\n",
    "for img, boxes_gt_xyxy, cids, _, img_id in dl:\n",
    "#     img, anchors, boxes, names, attns, n_head, boxes_pred_xyxy, cls_pred = examine.examine_attn(img, extractor, n_head, device, 'decoder.ca_layers.0.self_attn.q_proj', 'decoder.ca_layers.0.self_attn.k_proj')\n",
    "    img, anchors, boxes, names, attns, n_head, boxes_pred_xyxy, cls_pred = examine.examine_attn(img, extractor, n_head, device, 'decoder.ca_layers.7.self_attn.q_proj', 'decoder.ca_layers.7.self_attn.k_proj')\n",
    "    examine.draw_attn(img, anchors, boxes, names, attns, n_head)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032913b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img_ids = ['8314']\n",
    "# img_ids = ['565886']\n",
    "\n",
    "imgs = []\n",
    "for img_id in img_ids:\n",
    "    img, _, _, _, _ = ds.get_gt_by_img_id(img_id, dicts_train, train_img_dir, img_size, random_shift=False, n_query=model.decoder.n_anchor)\n",
    "    imgs.append(img)\n",
    "imgs = torch.stack(imgs, dim=0)\n",
    "\n",
    "n_head = 4\n",
    "\n",
    "# img, anchors, boxes, names, attns, n_head, boxes_pred_xyxy, cls_pred = examine.examine_attn(imgs, extractor, 4, device,\n",
    "#                                                                                             'decoder.attn1.q_proj', 'decoder.attn1.k_proj')\n",
    "img, anchors, boxes, names, attns, n_head, boxes_pred_xyxy, cls_pred = examine.examine_attn(imgs, extractor, n_head, device,\n",
    "                                                                                            'decoder.ca_layers.0.self_attn.q_proj', 'decoder.ca_layers.0.self_attn.k_proj')\n",
    "examine.draw_attn(img, anchors, boxes, names, attns, n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "403dd5a6-b92e-401d-ac5f-09ccd159e7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-4.2834, device='mps:0') tensor(3.1636, device='mps:0') tensor(-0.0185, device='mps:0') tensor(0.7020, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "out = extractor._features['decoder.decoder_layers.5.ffn.c_proj'][0, :, :384]\n",
    "out_min = out.min()\n",
    "out_max = out.max()\n",
    "print(out_min, out_max, out.mean(), out.std())\n",
    "out = (out - out_min) / (out_max - out_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba71af-42b2-4075-bd87-a183ce9167a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ds.OdDataset(dicts_val, n_query=model.decoder.n_anchor, train=False, sample_num=10, random_shift=False)\n",
    "dl = DataLoader(data, batch_size=2, shuffle=True)\n",
    "\n",
    "for img, boxes_gt_xyxy, cids, _, img_id in dl:\n",
    "    boxes_pred_xyxy, cls_pred = pred_and_show(img, model, thresh=0.1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23339202-654f-4278-97f0-eb0366822127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cnn\n",
      "cnn_ln\n",
      "encoder\n",
      "encoder.pos_emb_m\n",
      "encoder.pos_emb_m.ln\n",
      "encoder.attn_layers\n",
      "encoder.attn_layers.0\n",
      "encoder.attn_layers.0.q_ln\n",
      "encoder.attn_layers.0.k_ln\n",
      "encoder.attn_layers.0.v_ln\n",
      "encoder.attn_layers.0.out_ln\n",
      "encoder.attn_layers.0.self_attn\n",
      "encoder.attn_layers.0.self_attn.q_proj\n",
      "encoder.attn_layers.0.self_attn.k_proj\n",
      "encoder.attn_layers.0.self_attn.v_proj\n",
      "encoder.attn_layers.0.self_attn.out_proj\n",
      "encoder.attn_layers.0.ffn\n",
      "encoder.attn_layers.0.ffn.c_fc\n",
      "encoder.attn_layers.0.ffn.gelu\n",
      "encoder.attn_layers.0.ffn.c_proj\n",
      "encoder.attn_layers.1\n",
      "encoder.attn_layers.1.q_ln\n",
      "encoder.attn_layers.1.k_ln\n",
      "encoder.attn_layers.1.v_ln\n",
      "encoder.attn_layers.1.out_ln\n",
      "encoder.attn_layers.1.self_attn\n",
      "encoder.attn_layers.1.self_attn.q_proj\n",
      "encoder.attn_layers.1.self_attn.k_proj\n",
      "encoder.attn_layers.1.self_attn.v_proj\n",
      "encoder.attn_layers.1.self_attn.out_proj\n",
      "encoder.attn_layers.1.ffn\n",
      "encoder.attn_layers.1.ffn.c_fc\n",
      "encoder.attn_layers.1.ffn.gelu\n",
      "encoder.attn_layers.1.ffn.c_proj\n",
      "encoder.attn_layers.2\n",
      "encoder.attn_layers.2.q_ln\n",
      "encoder.attn_layers.2.k_ln\n",
      "encoder.attn_layers.2.v_ln\n",
      "encoder.attn_layers.2.out_ln\n",
      "encoder.attn_layers.2.self_attn\n",
      "encoder.attn_layers.2.self_attn.q_proj\n",
      "encoder.attn_layers.2.self_attn.k_proj\n",
      "encoder.attn_layers.2.self_attn.v_proj\n",
      "encoder.attn_layers.2.self_attn.out_proj\n",
      "encoder.attn_layers.2.ffn\n",
      "encoder.attn_layers.2.ffn.c_fc\n",
      "encoder.attn_layers.2.ffn.gelu\n",
      "encoder.attn_layers.2.ffn.c_proj\n",
      "encoder.attn_layers.3\n",
      "encoder.attn_layers.3.q_ln\n",
      "encoder.attn_layers.3.k_ln\n",
      "encoder.attn_layers.3.v_ln\n",
      "encoder.attn_layers.3.out_ln\n",
      "encoder.attn_layers.3.self_attn\n",
      "encoder.attn_layers.3.self_attn.q_proj\n",
      "encoder.attn_layers.3.self_attn.k_proj\n",
      "encoder.attn_layers.3.self_attn.v_proj\n",
      "encoder.attn_layers.3.self_attn.out_proj\n",
      "encoder.attn_layers.3.ffn\n",
      "encoder.attn_layers.3.ffn.c_fc\n",
      "encoder.attn_layers.3.ffn.gelu\n",
      "encoder.attn_layers.3.ffn.c_proj\n",
      "encoder.attn_layers.4\n",
      "encoder.attn_layers.4.q_ln\n",
      "encoder.attn_layers.4.k_ln\n",
      "encoder.attn_layers.4.v_ln\n",
      "encoder.attn_layers.4.out_ln\n",
      "encoder.attn_layers.4.self_attn\n",
      "encoder.attn_layers.4.self_attn.q_proj\n",
      "encoder.attn_layers.4.self_attn.k_proj\n",
      "encoder.attn_layers.4.self_attn.v_proj\n",
      "encoder.attn_layers.4.self_attn.out_proj\n",
      "encoder.attn_layers.4.ffn\n",
      "encoder.attn_layers.4.ffn.c_fc\n",
      "encoder.attn_layers.4.ffn.gelu\n",
      "encoder.attn_layers.4.ffn.c_proj\n",
      "encoder.attn_layers.5\n",
      "encoder.attn_layers.5.q_ln\n",
      "encoder.attn_layers.5.k_ln\n",
      "encoder.attn_layers.5.v_ln\n",
      "encoder.attn_layers.5.out_ln\n",
      "encoder.attn_layers.5.self_attn\n",
      "encoder.attn_layers.5.self_attn.q_proj\n",
      "encoder.attn_layers.5.self_attn.k_proj\n",
      "encoder.attn_layers.5.self_attn.v_proj\n",
      "encoder.attn_layers.5.self_attn.out_proj\n",
      "encoder.attn_layers.5.ffn\n",
      "encoder.attn_layers.5.ffn.c_fc\n",
      "encoder.attn_layers.5.ffn.gelu\n",
      "encoder.attn_layers.5.ffn.c_proj\n",
      "encoder.attn_layers.6\n",
      "encoder.attn_layers.6.q_ln\n",
      "encoder.attn_layers.6.k_ln\n",
      "encoder.attn_layers.6.v_ln\n",
      "encoder.attn_layers.6.out_ln\n",
      "encoder.attn_layers.6.self_attn\n",
      "encoder.attn_layers.6.self_attn.q_proj\n",
      "encoder.attn_layers.6.self_attn.k_proj\n",
      "encoder.attn_layers.6.self_attn.v_proj\n",
      "encoder.attn_layers.6.self_attn.out_proj\n",
      "encoder.attn_layers.6.ffn\n",
      "encoder.attn_layers.6.ffn.c_fc\n",
      "encoder.attn_layers.6.ffn.gelu\n",
      "encoder.attn_layers.6.ffn.c_proj\n",
      "encoder.attn_layers.7\n",
      "encoder.attn_layers.7.q_ln\n",
      "encoder.attn_layers.7.k_ln\n",
      "encoder.attn_layers.7.v_ln\n",
      "encoder.attn_layers.7.out_ln\n",
      "encoder.attn_layers.7.self_attn\n",
      "encoder.attn_layers.7.self_attn.q_proj\n",
      "encoder.attn_layers.7.self_attn.k_proj\n",
      "encoder.attn_layers.7.self_attn.v_proj\n",
      "encoder.attn_layers.7.self_attn.out_proj\n",
      "encoder.attn_layers.7.ffn\n",
      "encoder.attn_layers.7.ffn.c_fc\n",
      "encoder.attn_layers.7.ffn.gelu\n",
      "encoder.attn_layers.7.ffn.c_proj\n",
      "encoder.attn_layers.8\n",
      "encoder.attn_layers.8.q_ln\n",
      "encoder.attn_layers.8.k_ln\n",
      "encoder.attn_layers.8.v_ln\n",
      "encoder.attn_layers.8.out_ln\n",
      "encoder.attn_layers.8.self_attn\n",
      "encoder.attn_layers.8.self_attn.q_proj\n",
      "encoder.attn_layers.8.self_attn.k_proj\n",
      "encoder.attn_layers.8.self_attn.v_proj\n",
      "encoder.attn_layers.8.self_attn.out_proj\n",
      "encoder.attn_layers.8.ffn\n",
      "encoder.attn_layers.8.ffn.c_fc\n",
      "encoder.attn_layers.8.ffn.gelu\n",
      "encoder.attn_layers.8.ffn.c_proj\n",
      "encoder.attn_layers.9\n",
      "encoder.attn_layers.9.q_ln\n",
      "encoder.attn_layers.9.k_ln\n",
      "encoder.attn_layers.9.v_ln\n",
      "encoder.attn_layers.9.out_ln\n",
      "encoder.attn_layers.9.self_attn\n",
      "encoder.attn_layers.9.self_attn.q_proj\n",
      "encoder.attn_layers.9.self_attn.k_proj\n",
      "encoder.attn_layers.9.self_attn.v_proj\n",
      "encoder.attn_layers.9.self_attn.out_proj\n",
      "encoder.attn_layers.9.ffn\n",
      "encoder.attn_layers.9.ffn.c_fc\n",
      "encoder.attn_layers.9.ffn.gelu\n",
      "encoder.attn_layers.9.ffn.c_proj\n",
      "encoder.attn_layers.10\n",
      "encoder.attn_layers.10.q_ln\n",
      "encoder.attn_layers.10.k_ln\n",
      "encoder.attn_layers.10.v_ln\n",
      "encoder.attn_layers.10.out_ln\n",
      "encoder.attn_layers.10.self_attn\n",
      "encoder.attn_layers.10.self_attn.q_proj\n",
      "encoder.attn_layers.10.self_attn.k_proj\n",
      "encoder.attn_layers.10.self_attn.v_proj\n",
      "encoder.attn_layers.10.self_attn.out_proj\n",
      "encoder.attn_layers.10.ffn\n",
      "encoder.attn_layers.10.ffn.c_fc\n",
      "encoder.attn_layers.10.ffn.gelu\n",
      "encoder.attn_layers.10.ffn.c_proj\n",
      "encoder.attn_layers.11\n",
      "encoder.attn_layers.11.q_ln\n",
      "encoder.attn_layers.11.k_ln\n",
      "encoder.attn_layers.11.v_ln\n",
      "encoder.attn_layers.11.out_ln\n",
      "encoder.attn_layers.11.self_attn\n",
      "encoder.attn_layers.11.self_attn.q_proj\n",
      "encoder.attn_layers.11.self_attn.k_proj\n",
      "encoder.attn_layers.11.self_attn.v_proj\n",
      "encoder.attn_layers.11.self_attn.out_proj\n",
      "encoder.attn_layers.11.ffn\n",
      "encoder.attn_layers.11.ffn.c_fc\n",
      "encoder.attn_layers.11.ffn.gelu\n",
      "encoder.attn_layers.11.ffn.c_proj\n",
      "encoder.attn_layers.12\n",
      "encoder.attn_layers.12.q_ln\n",
      "encoder.attn_layers.12.k_ln\n",
      "encoder.attn_layers.12.v_ln\n",
      "encoder.attn_layers.12.out_ln\n",
      "encoder.attn_layers.12.self_attn\n",
      "encoder.attn_layers.12.self_attn.q_proj\n",
      "encoder.attn_layers.12.self_attn.k_proj\n",
      "encoder.attn_layers.12.self_attn.v_proj\n",
      "encoder.attn_layers.12.self_attn.out_proj\n",
      "encoder.attn_layers.12.ffn\n",
      "encoder.attn_layers.12.ffn.c_fc\n",
      "encoder.attn_layers.12.ffn.gelu\n",
      "encoder.attn_layers.12.ffn.c_proj\n",
      "encoder.attn_layers.13\n",
      "encoder.attn_layers.13.q_ln\n",
      "encoder.attn_layers.13.k_ln\n",
      "encoder.attn_layers.13.v_ln\n",
      "encoder.attn_layers.13.out_ln\n",
      "encoder.attn_layers.13.self_attn\n",
      "encoder.attn_layers.13.self_attn.q_proj\n",
      "encoder.attn_layers.13.self_attn.k_proj\n",
      "encoder.attn_layers.13.self_attn.v_proj\n",
      "encoder.attn_layers.13.self_attn.out_proj\n",
      "encoder.attn_layers.13.ffn\n",
      "encoder.attn_layers.13.ffn.c_fc\n",
      "encoder.attn_layers.13.ffn.gelu\n",
      "encoder.attn_layers.13.ffn.c_proj\n",
      "encoder.attn_layers.14\n",
      "encoder.attn_layers.14.q_ln\n",
      "encoder.attn_layers.14.k_ln\n",
      "encoder.attn_layers.14.v_ln\n",
      "encoder.attn_layers.14.out_ln\n",
      "encoder.attn_layers.14.self_attn\n",
      "encoder.attn_layers.14.self_attn.q_proj\n",
      "encoder.attn_layers.14.self_attn.k_proj\n",
      "encoder.attn_layers.14.self_attn.v_proj\n",
      "encoder.attn_layers.14.self_attn.out_proj\n",
      "encoder.attn_layers.14.ffn\n",
      "encoder.attn_layers.14.ffn.c_fc\n",
      "encoder.attn_layers.14.ffn.gelu\n",
      "encoder.attn_layers.14.ffn.c_proj\n",
      "encoder.attn_layers.15\n",
      "encoder.attn_layers.15.q_ln\n",
      "encoder.attn_layers.15.k_ln\n",
      "encoder.attn_layers.15.v_ln\n",
      "encoder.attn_layers.15.out_ln\n",
      "encoder.attn_layers.15.self_attn\n",
      "encoder.attn_layers.15.self_attn.q_proj\n",
      "encoder.attn_layers.15.self_attn.k_proj\n",
      "encoder.attn_layers.15.self_attn.v_proj\n",
      "encoder.attn_layers.15.self_attn.out_proj\n",
      "encoder.attn_layers.15.ffn\n",
      "encoder.attn_layers.15.ffn.c_fc\n",
      "encoder.attn_layers.15.ffn.gelu\n",
      "encoder.attn_layers.15.ffn.c_proj\n",
      "encoder.attn_layers.16\n",
      "encoder.attn_layers.16.q_ln\n",
      "encoder.attn_layers.16.k_ln\n",
      "encoder.attn_layers.16.v_ln\n",
      "encoder.attn_layers.16.out_ln\n",
      "encoder.attn_layers.16.self_attn\n",
      "encoder.attn_layers.16.self_attn.q_proj\n",
      "encoder.attn_layers.16.self_attn.k_proj\n",
      "encoder.attn_layers.16.self_attn.v_proj\n",
      "encoder.attn_layers.16.self_attn.out_proj\n",
      "encoder.attn_layers.16.ffn\n",
      "encoder.attn_layers.16.ffn.c_fc\n",
      "encoder.attn_layers.16.ffn.gelu\n",
      "encoder.attn_layers.16.ffn.c_proj\n",
      "encoder.attn_layers.17\n",
      "encoder.attn_layers.17.q_ln\n",
      "encoder.attn_layers.17.k_ln\n",
      "encoder.attn_layers.17.v_ln\n",
      "encoder.attn_layers.17.out_ln\n",
      "encoder.attn_layers.17.self_attn\n",
      "encoder.attn_layers.17.self_attn.q_proj\n",
      "encoder.attn_layers.17.self_attn.k_proj\n",
      "encoder.attn_layers.17.self_attn.v_proj\n",
      "encoder.attn_layers.17.self_attn.out_proj\n",
      "encoder.attn_layers.17.ffn\n",
      "encoder.attn_layers.17.ffn.c_fc\n",
      "encoder.attn_layers.17.ffn.gelu\n",
      "encoder.attn_layers.17.ffn.c_proj\n",
      "encoder.attn_layers.18\n",
      "encoder.attn_layers.18.q_ln\n",
      "encoder.attn_layers.18.k_ln\n",
      "encoder.attn_layers.18.v_ln\n",
      "encoder.attn_layers.18.out_ln\n",
      "encoder.attn_layers.18.self_attn\n",
      "encoder.attn_layers.18.self_attn.q_proj\n",
      "encoder.attn_layers.18.self_attn.k_proj\n",
      "encoder.attn_layers.18.self_attn.v_proj\n",
      "encoder.attn_layers.18.self_attn.out_proj\n",
      "encoder.attn_layers.18.ffn\n",
      "encoder.attn_layers.18.ffn.c_fc\n",
      "encoder.attn_layers.18.ffn.gelu\n",
      "encoder.attn_layers.18.ffn.c_proj\n",
      "encoder.attn_layers.19\n",
      "encoder.attn_layers.19.q_ln\n",
      "encoder.attn_layers.19.k_ln\n",
      "encoder.attn_layers.19.v_ln\n",
      "encoder.attn_layers.19.out_ln\n",
      "encoder.attn_layers.19.self_attn\n",
      "encoder.attn_layers.19.self_attn.q_proj\n",
      "encoder.attn_layers.19.self_attn.k_proj\n",
      "encoder.attn_layers.19.self_attn.v_proj\n",
      "encoder.attn_layers.19.self_attn.out_proj\n",
      "encoder.attn_layers.19.ffn\n",
      "encoder.attn_layers.19.ffn.c_fc\n",
      "encoder.attn_layers.19.ffn.gelu\n",
      "encoder.attn_layers.19.ffn.c_proj\n",
      "encoder.attn_layers.20\n",
      "encoder.attn_layers.20.q_ln\n",
      "encoder.attn_layers.20.k_ln\n",
      "encoder.attn_layers.20.v_ln\n",
      "encoder.attn_layers.20.out_ln\n",
      "encoder.attn_layers.20.self_attn\n",
      "encoder.attn_layers.20.self_attn.q_proj\n",
      "encoder.attn_layers.20.self_attn.k_proj\n",
      "encoder.attn_layers.20.self_attn.v_proj\n",
      "encoder.attn_layers.20.self_attn.out_proj\n",
      "encoder.attn_layers.20.ffn\n",
      "encoder.attn_layers.20.ffn.c_fc\n",
      "encoder.attn_layers.20.ffn.gelu\n",
      "encoder.attn_layers.20.ffn.c_proj\n",
      "encoder.attn_layers.21\n",
      "encoder.attn_layers.21.q_ln\n",
      "encoder.attn_layers.21.k_ln\n",
      "encoder.attn_layers.21.v_ln\n",
      "encoder.attn_layers.21.out_ln\n",
      "encoder.attn_layers.21.self_attn\n",
      "encoder.attn_layers.21.self_attn.q_proj\n",
      "encoder.attn_layers.21.self_attn.k_proj\n",
      "encoder.attn_layers.21.self_attn.v_proj\n",
      "encoder.attn_layers.21.self_attn.out_proj\n",
      "encoder.attn_layers.21.ffn\n",
      "encoder.attn_layers.21.ffn.c_fc\n",
      "encoder.attn_layers.21.ffn.gelu\n",
      "encoder.attn_layers.21.ffn.c_proj\n",
      "encoder.attn_layers.22\n",
      "encoder.attn_layers.22.q_ln\n",
      "encoder.attn_layers.22.k_ln\n",
      "encoder.attn_layers.22.v_ln\n",
      "encoder.attn_layers.22.out_ln\n",
      "encoder.attn_layers.22.self_attn\n",
      "encoder.attn_layers.22.self_attn.q_proj\n",
      "encoder.attn_layers.22.self_attn.k_proj\n",
      "encoder.attn_layers.22.self_attn.v_proj\n",
      "encoder.attn_layers.22.self_attn.out_proj\n",
      "encoder.attn_layers.22.ffn\n",
      "encoder.attn_layers.22.ffn.c_fc\n",
      "encoder.attn_layers.22.ffn.gelu\n",
      "encoder.attn_layers.22.ffn.c_proj\n",
      "encoder.attn_layers.23\n",
      "encoder.attn_layers.23.q_ln\n",
      "encoder.attn_layers.23.k_ln\n",
      "encoder.attn_layers.23.v_ln\n",
      "encoder.attn_layers.23.out_ln\n",
      "encoder.attn_layers.23.self_attn\n",
      "encoder.attn_layers.23.self_attn.q_proj\n",
      "encoder.attn_layers.23.self_attn.k_proj\n",
      "encoder.attn_layers.23.self_attn.v_proj\n",
      "encoder.attn_layers.23.self_attn.out_proj\n",
      "encoder.attn_layers.23.ffn\n",
      "encoder.attn_layers.23.ffn.c_fc\n",
      "encoder.attn_layers.23.ffn.gelu\n",
      "encoder.attn_layers.23.ffn.c_proj\n",
      "decoder\n",
      "decoder.pos_emb_m\n",
      "decoder.pos_emb_m.ln\n",
      "decoder.ca_layers\n",
      "decoder.ca_layers.0\n",
      "decoder.ca_layers.0.q_ln\n",
      "decoder.ca_layers.0.k_ln\n",
      "decoder.ca_layers.0.v_ln\n",
      "decoder.ca_layers.0.out_ln\n",
      "decoder.ca_layers.0.self_attn\n",
      "decoder.ca_layers.0.self_attn.q_proj\n",
      "decoder.ca_layers.0.self_attn.k_proj\n",
      "decoder.ca_layers.0.self_attn.v_proj\n",
      "decoder.ca_layers.0.self_attn.out_proj\n",
      "decoder.ca_layers.0.ffn\n",
      "decoder.ca_layers.0.ffn.c_fc\n",
      "decoder.ca_layers.0.ffn.gelu\n",
      "decoder.ca_layers.0.ffn.c_proj\n",
      "decoder.ca_layers.1\n",
      "decoder.ca_layers.1.q_ln\n",
      "decoder.ca_layers.1.k_ln\n",
      "decoder.ca_layers.1.v_ln\n",
      "decoder.ca_layers.1.out_ln\n",
      "decoder.ca_layers.1.self_attn\n",
      "decoder.ca_layers.1.self_attn.q_proj\n",
      "decoder.ca_layers.1.self_attn.k_proj\n",
      "decoder.ca_layers.1.self_attn.v_proj\n",
      "decoder.ca_layers.1.self_attn.out_proj\n",
      "decoder.ca_layers.1.ffn\n",
      "decoder.ca_layers.1.ffn.c_fc\n",
      "decoder.ca_layers.1.ffn.gelu\n",
      "decoder.ca_layers.1.ffn.c_proj\n",
      "decoder.ca_layers.2\n",
      "decoder.ca_layers.2.q_ln\n",
      "decoder.ca_layers.2.k_ln\n",
      "decoder.ca_layers.2.v_ln\n",
      "decoder.ca_layers.2.out_ln\n",
      "decoder.ca_layers.2.self_attn\n",
      "decoder.ca_layers.2.self_attn.q_proj\n",
      "decoder.ca_layers.2.self_attn.k_proj\n",
      "decoder.ca_layers.2.self_attn.v_proj\n",
      "decoder.ca_layers.2.self_attn.out_proj\n",
      "decoder.ca_layers.2.ffn\n",
      "decoder.ca_layers.2.ffn.c_fc\n",
      "decoder.ca_layers.2.ffn.gelu\n",
      "decoder.ca_layers.2.ffn.c_proj\n",
      "decoder.ca_layers.3\n",
      "decoder.ca_layers.3.q_ln\n",
      "decoder.ca_layers.3.k_ln\n",
      "decoder.ca_layers.3.v_ln\n",
      "decoder.ca_layers.3.out_ln\n",
      "decoder.ca_layers.3.self_attn\n",
      "decoder.ca_layers.3.self_attn.q_proj\n",
      "decoder.ca_layers.3.self_attn.k_proj\n",
      "decoder.ca_layers.3.self_attn.v_proj\n",
      "decoder.ca_layers.3.self_attn.out_proj\n",
      "decoder.ca_layers.3.ffn\n",
      "decoder.ca_layers.3.ffn.c_fc\n",
      "decoder.ca_layers.3.ffn.gelu\n",
      "decoder.ca_layers.3.ffn.c_proj\n",
      "decoder.ca_layers.4\n",
      "decoder.ca_layers.4.q_ln\n",
      "decoder.ca_layers.4.k_ln\n",
      "decoder.ca_layers.4.v_ln\n",
      "decoder.ca_layers.4.out_ln\n",
      "decoder.ca_layers.4.self_attn\n",
      "decoder.ca_layers.4.self_attn.q_proj\n",
      "decoder.ca_layers.4.self_attn.k_proj\n",
      "decoder.ca_layers.4.self_attn.v_proj\n",
      "decoder.ca_layers.4.self_attn.out_proj\n",
      "decoder.ca_layers.4.ffn\n",
      "decoder.ca_layers.4.ffn.c_fc\n",
      "decoder.ca_layers.4.ffn.gelu\n",
      "decoder.ca_layers.4.ffn.c_proj\n",
      "decoder.ca_layers.5\n",
      "decoder.ca_layers.5.q_ln\n",
      "decoder.ca_layers.5.k_ln\n",
      "decoder.ca_layers.5.v_ln\n",
      "decoder.ca_layers.5.out_ln\n",
      "decoder.ca_layers.5.self_attn\n",
      "decoder.ca_layers.5.self_attn.q_proj\n",
      "decoder.ca_layers.5.self_attn.k_proj\n",
      "decoder.ca_layers.5.self_attn.v_proj\n",
      "decoder.ca_layers.5.self_attn.out_proj\n",
      "decoder.ca_layers.5.ffn\n",
      "decoder.ca_layers.5.ffn.c_fc\n",
      "decoder.ca_layers.5.ffn.gelu\n",
      "decoder.ca_layers.5.ffn.c_proj\n",
      "decoder.ca_layers.6\n",
      "decoder.ca_layers.6.q_ln\n",
      "decoder.ca_layers.6.k_ln\n",
      "decoder.ca_layers.6.v_ln\n",
      "decoder.ca_layers.6.out_ln\n",
      "decoder.ca_layers.6.self_attn\n",
      "decoder.ca_layers.6.self_attn.q_proj\n",
      "decoder.ca_layers.6.self_attn.k_proj\n",
      "decoder.ca_layers.6.self_attn.v_proj\n",
      "decoder.ca_layers.6.self_attn.out_proj\n",
      "decoder.ca_layers.6.ffn\n",
      "decoder.ca_layers.6.ffn.c_fc\n",
      "decoder.ca_layers.6.ffn.gelu\n",
      "decoder.ca_layers.6.ffn.c_proj\n",
      "decoder.ca_layers.7\n",
      "decoder.ca_layers.7.q_ln\n",
      "decoder.ca_layers.7.k_ln\n",
      "decoder.ca_layers.7.v_ln\n",
      "decoder.ca_layers.7.out_ln\n",
      "decoder.ca_layers.7.self_attn\n",
      "decoder.ca_layers.7.self_attn.q_proj\n",
      "decoder.ca_layers.7.self_attn.k_proj\n",
      "decoder.ca_layers.7.self_attn.v_proj\n",
      "decoder.ca_layers.7.self_attn.out_proj\n",
      "decoder.ca_layers.7.ffn\n",
      "decoder.ca_layers.7.ffn.c_fc\n",
      "decoder.ca_layers.7.ffn.gelu\n",
      "decoder.ca_layers.7.ffn.c_proj\n",
      "decoder.sa_layers\n",
      "decoder.sa_layers.0\n",
      "decoder.sa_layers.0.q_ln\n",
      "decoder.sa_layers.0.k_ln\n",
      "decoder.sa_layers.0.v_ln\n",
      "decoder.sa_layers.0.out_ln\n",
      "decoder.sa_layers.0.self_attn\n",
      "decoder.sa_layers.0.self_attn.q_proj\n",
      "decoder.sa_layers.0.self_attn.k_proj\n",
      "decoder.sa_layers.0.self_attn.v_proj\n",
      "decoder.sa_layers.0.self_attn.out_proj\n",
      "decoder.sa_layers.0.ffn\n",
      "decoder.sa_layers.0.ffn.c_fc\n",
      "decoder.sa_layers.0.ffn.gelu\n",
      "decoder.sa_layers.0.ffn.c_proj\n",
      "decoder.sa_layers.1\n",
      "decoder.sa_layers.1.q_ln\n",
      "decoder.sa_layers.1.k_ln\n",
      "decoder.sa_layers.1.v_ln\n",
      "decoder.sa_layers.1.out_ln\n",
      "decoder.sa_layers.1.self_attn\n",
      "decoder.sa_layers.1.self_attn.q_proj\n",
      "decoder.sa_layers.1.self_attn.k_proj\n",
      "decoder.sa_layers.1.self_attn.v_proj\n",
      "decoder.sa_layers.1.self_attn.out_proj\n",
      "decoder.sa_layers.1.ffn\n",
      "decoder.sa_layers.1.ffn.c_fc\n",
      "decoder.sa_layers.1.ffn.gelu\n",
      "decoder.sa_layers.1.ffn.c_proj\n",
      "decoder.sa_layers.2\n",
      "decoder.sa_layers.2.q_ln\n",
      "decoder.sa_layers.2.k_ln\n",
      "decoder.sa_layers.2.v_ln\n",
      "decoder.sa_layers.2.out_ln\n",
      "decoder.sa_layers.2.self_attn\n",
      "decoder.sa_layers.2.self_attn.q_proj\n",
      "decoder.sa_layers.2.self_attn.k_proj\n",
      "decoder.sa_layers.2.self_attn.v_proj\n",
      "decoder.sa_layers.2.self_attn.out_proj\n",
      "decoder.sa_layers.2.ffn\n",
      "decoder.sa_layers.2.ffn.c_fc\n",
      "decoder.sa_layers.2.ffn.gelu\n",
      "decoder.sa_layers.2.ffn.c_proj\n",
      "decoder.sa_layers.3\n",
      "decoder.sa_layers.3.q_ln\n",
      "decoder.sa_layers.3.k_ln\n",
      "decoder.sa_layers.3.v_ln\n",
      "decoder.sa_layers.3.out_ln\n",
      "decoder.sa_layers.3.self_attn\n",
      "decoder.sa_layers.3.self_attn.q_proj\n",
      "decoder.sa_layers.3.self_attn.k_proj\n",
      "decoder.sa_layers.3.self_attn.v_proj\n",
      "decoder.sa_layers.3.self_attn.out_proj\n",
      "decoder.sa_layers.3.ffn\n",
      "decoder.sa_layers.3.ffn.c_fc\n",
      "decoder.sa_layers.3.ffn.gelu\n",
      "decoder.sa_layers.3.ffn.c_proj\n",
      "decoder.sa_layers.4\n",
      "decoder.sa_layers.4.q_ln\n",
      "decoder.sa_layers.4.k_ln\n",
      "decoder.sa_layers.4.v_ln\n",
      "decoder.sa_layers.4.out_ln\n",
      "decoder.sa_layers.4.self_attn\n",
      "decoder.sa_layers.4.self_attn.q_proj\n",
      "decoder.sa_layers.4.self_attn.k_proj\n",
      "decoder.sa_layers.4.self_attn.v_proj\n",
      "decoder.sa_layers.4.self_attn.out_proj\n",
      "decoder.sa_layers.4.ffn\n",
      "decoder.sa_layers.4.ffn.c_fc\n",
      "decoder.sa_layers.4.ffn.gelu\n",
      "decoder.sa_layers.4.ffn.c_proj\n",
      "decoder.sa_layers.5\n",
      "decoder.sa_layers.5.q_ln\n",
      "decoder.sa_layers.5.k_ln\n",
      "decoder.sa_layers.5.v_ln\n",
      "decoder.sa_layers.5.out_ln\n",
      "decoder.sa_layers.5.self_attn\n",
      "decoder.sa_layers.5.self_attn.q_proj\n",
      "decoder.sa_layers.5.self_attn.k_proj\n",
      "decoder.sa_layers.5.self_attn.v_proj\n",
      "decoder.sa_layers.5.self_attn.out_proj\n",
      "decoder.sa_layers.5.ffn\n",
      "decoder.sa_layers.5.ffn.c_fc\n",
      "decoder.sa_layers.5.ffn.gelu\n",
      "decoder.sa_layers.5.ffn.c_proj\n",
      "decoder.sa_layers.6\n",
      "decoder.sa_layers.6.q_ln\n",
      "decoder.sa_layers.6.k_ln\n",
      "decoder.sa_layers.6.v_ln\n",
      "decoder.sa_layers.6.out_ln\n",
      "decoder.sa_layers.6.self_attn\n",
      "decoder.sa_layers.6.self_attn.q_proj\n",
      "decoder.sa_layers.6.self_attn.k_proj\n",
      "decoder.sa_layers.6.self_attn.v_proj\n",
      "decoder.sa_layers.6.self_attn.out_proj\n",
      "decoder.sa_layers.6.ffn\n",
      "decoder.sa_layers.6.ffn.c_fc\n",
      "decoder.sa_layers.6.ffn.gelu\n",
      "decoder.sa_layers.6.ffn.c_proj\n",
      "decoder.sa_layers.7\n",
      "decoder.sa_layers.7.q_ln\n",
      "decoder.sa_layers.7.k_ln\n",
      "decoder.sa_layers.7.v_ln\n",
      "decoder.sa_layers.7.out_ln\n",
      "decoder.sa_layers.7.self_attn\n",
      "decoder.sa_layers.7.self_attn.q_proj\n",
      "decoder.sa_layers.7.self_attn.k_proj\n",
      "decoder.sa_layers.7.self_attn.v_proj\n",
      "decoder.sa_layers.7.self_attn.out_proj\n",
      "decoder.sa_layers.7.ffn\n",
      "decoder.sa_layers.7.ffn.c_fc\n",
      "decoder.sa_layers.7.ffn.gelu\n",
      "decoder.sa_layers.7.ffn.c_proj\n",
      "decoder.anchor_shift_reg\n",
      "decoder.anchor_shift_reg.layers\n",
      "decoder.anchor_shift_reg.layers.0\n",
      "decoder.anchor_shift_reg.layers.1\n",
      "decoder.ln\n",
      "decoder.cls_reg\n",
      "decoder.cls_reg.layers\n",
      "decoder.cls_reg.layers.0\n",
      "decoder.cls_reg.layers.1\n"
     ]
    }
   ],
   "source": [
    "for m in model.named_modules():\n",
    "    print(m[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6f4292cf-d180-4b8a-a469-45dffbf6c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones([1, 90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6e7657d-37b7-4f98-9a69-efcfc4d08554",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0, 10:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fcb7e320-4f42-4fea-b6ac-7d988d304343",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = torch.nn.LayerNorm(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8c0f0710-b359-4eac-aece-586aa8bb0288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.8283,  2.8283,  2.8283,  2.8283,  2.8283,  2.8283,  2.8283,  2.8283,\n",
       "          2.8283,  2.8283, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535, -0.3535,\n",
       "         -0.3535, -0.3535]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b265278-6b9f-4cd9-b65d-61450a2751ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
